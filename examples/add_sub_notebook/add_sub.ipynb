{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of using Triton Server Wrapper in Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton server setup with custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (1.23.5)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting cupy-cuda115\n",
      "  Using cached cupy_cuda115-10.6.0-cp38-cp38-manylinux1_x86_64.whl (83.3 MB)\n",
      "Requirement already satisfied: numpy<1.25,>=1.18 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from cupy-cuda115) (1.23.5)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda115)\n",
      "  Using cached fastrlock-0.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_24_x86_64.whl (48 kB)\n",
      "Installing collected packages: fastrlock, cupy-cuda115\n",
      "Successfully installed cupy-cuda115-10.6.0 fastrlock-0.8.1\n",
      "Collecting nvidia-pytriton\n",
      "  Using cached nvidia_pytriton-0.1.4-py3-none-manylinux_2_31_x86_64.whl (36.7 MB)\n",
      "Requirement already satisfied: numpy~=1.21 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from nvidia-pytriton) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.7.0 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from nvidia-pytriton) (3.20.2)\n",
      "Collecting pyzmq~=23.0 (from nvidia-pytriton)\n",
      "  Downloading pyzmq-23.2.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sh~=1.14 (from nvidia-pytriton)\n",
      "  Using cached sh-1.14.3.tar.gz (62 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tritonclient[all] (from nvidia-pytriton)\n",
      "  Using cached tritonclient-2.33.0-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting typing-inspect~=0.6.0 (from nvidia-pytriton)\n",
      "  Using cached typing_inspect-0.6.0-py3-none-any.whl (8.1 kB)\n",
      "Requirement already satisfied: wrapt~=1.14 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from nvidia-pytriton) (1.14.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from typing-inspect~=0.6.0->nvidia-pytriton) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from typing-inspect~=0.6.0->nvidia-pytriton) (4.5.0)\n",
      "Collecting python-rapidjson>=0.9.1 (from tritonclient[all]->nvidia-pytriton)\n",
      "  Downloading python_rapidjson-1.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio>=1.41.0 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from tritonclient[all]->nvidia-pytriton) (1.54.0)\n",
      "Requirement already satisfied: packaging>=14.1 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from tritonclient[all]->nvidia-pytriton) (23.1)\n",
      "Collecting geventhttpclient<=2.0.2,>=1.4.4 (from tritonclient[all]->nvidia-pytriton)\n",
      "  Downloading geventhttpclient-2.0.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp>=3.8.1 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from tritonclient[all]->nvidia-pytriton) (3.8.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[all]->nvidia-pytriton) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[all]->nvidia-pytriton) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[all]->nvidia-pytriton) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[all]->nvidia-pytriton) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[all]->nvidia-pytriton) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[all]->nvidia-pytriton) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from aiohttp>=3.8.1->tritonclient[all]->nvidia-pytriton) (1.3.1)\n",
      "Collecting gevent>=0.13 (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->nvidia-pytriton)\n",
      "  Downloading gevent-22.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->nvidia-pytriton) (2022.12.7)\n",
      "Requirement already satisfied: six in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->nvidia-pytriton) (1.16.0)\n",
      "Collecting brotli (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->nvidia-pytriton)\n",
      "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zope.event (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->nvidia-pytriton)\n",
      "  Using cached zope.event-4.6-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting zope.interface (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->nvidia-pytriton)\n",
      "  Downloading zope.interface-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->nvidia-pytriton) (44.0.0)\n",
      "Requirement already satisfied: greenlet>=2.0.0 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[all]->nvidia-pytriton) (2.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.1->tritonclient[all]->nvidia-pytriton) (3.4)\n",
      "Building wheels for collected packages: sh\n",
      "  Building wheel for sh (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sh: filename=sh-1.14.3-py2.py3-none-any.whl size=39636 sha256=0e331d89ad1253b137e8350569fa623973e7403dafbd309519d8ba81314fe839\n",
      "  Stored in directory: /home/Startupcolors/.cache/pip/wheels/3f/72/e2/2477a76f9fbf01a61b1dd9b34ff883288d6266d856461ba4d4\n",
      "Successfully built sh\n",
      "Installing collected packages: sh, brotli, zope.interface, zope.event, typing-inspect, pyzmq, python-rapidjson, tritonclient, gevent, geventhttpclient, nvidia-pytriton\n",
      "  Attempting uninstall: typing-inspect\n",
      "    Found existing installation: typing-inspect 0.8.0\n",
      "    Uninstalling typing-inspect-0.8.0:\n",
      "      Successfully uninstalled typing-inspect-0.8.0\n",
      "  Attempting uninstall: pyzmq\n",
      "    Found existing installation: pyzmq 25.0.2\n",
      "    Uninstalling pyzmq-25.0.2:\n",
      "      Successfully uninstalled pyzmq-25.0.2\n",
      "Successfully installed brotli-1.0.9 gevent-22.10.2 geventhttpclient-2.0.2 nvidia-pytriton-0.1.4 python-rapidjson-1.10 pyzmq-23.2.1 sh-1.14.3 tritonclient-2.33.0 typing-inspect-0.6.0 zope.event-4.6 zope.interface-6.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install cupy-cuda115 --extra-index-url=https://pypi.ngc.nvidia.com\n",
    "!pip install -U nvidia-pytriton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pytriton.model_config import ModelConfig, Tensor\n",
    "from pytriton.triton import Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define inference callable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_sub(**inputs):\n",
    "    a_batch, b_batch = inputs.values()\n",
    "    add_batch = a_batch + b_batch\n",
    "    sub_batch = a_batch - b_batch\n",
    "    return {\"add\": add_batch, \"sub\": sub_batch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate titon wrapper class and load model with defined callable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton = Triton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton.bind(\n",
    "        model_name=\"AddSub\",\n",
    "        infer_func=_add_sub,\n",
    "        inputs=[\n",
    "            Tensor(dtype=np.float32, shape=(-1,)),\n",
    "            Tensor(dtype=np.float32, shape=(-1,)),\n",
    "        ],\n",
    "        outputs=[\n",
    "            Tensor(name=\"add\", dtype=np.float32, shape=(-1,)),\n",
    "            Tensor(name=\"sub\", dtype=np.float32, shape=(-1,)),\n",
    "        ],\n",
    "        config=ModelConfig(max_batch_size=128),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run triton server with defined model inference callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I0511 08:33:07.682040 36642 pinned_memory_manager.cc:240] Pinned memory pool is created at '0x7f41f4000000' with size 268435456\n",
      "I0511 08:33:07.682708 36642 cuda_memory_manager.cc:105] CUDA memory pool is created on device 0 with size 67108864\n",
      "I0511 08:33:07.687473 36642 model_lifecycle.cc:459] loading: AddSub:1\n",
      "I0511 08:33:09.017897 36642 python_be.cc:1856] TRITONBACKEND_ModelInstanceInitialize: AddSub_0 (CPU device 0)\n",
      "I0511 08:33:09.210668 36642 model_lifecycle.cc:694] successfully loaded 'AddSub' version 1\n",
      "I0511 08:33:09.210796 36642 server.cc:563] \n",
      "+------------------+------+\n",
      "| Repository Agent | Path |\n",
      "+------------------+------+\n",
      "+------------------+------+\n",
      "\n",
      "I0511 08:33:09.210875 36642 server.cc:590] \n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| Backend | Path                            | Config                          |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "| python  | /home/Startupcolors/doze/wound/ | {\"cmdline\":{\"auto-complete-conf |\n",
      "|         | notebooks/.venv/lib/python3.8/s | ig\":\"true\",\"min-compute-capabil |\n",
      "|         | ite-packages/pytriton/tritonser | ity\":\"6.000000\",\"backend-direct |\n",
      "|         | ver/backends/python/libtriton_p | ory\":\"/home/Startupcolors/doze/ |\n",
      "|         | ython.so                        | wound/notebooks/.venv/lib/pytho |\n",
      "|         |                                 | n3.8/site-packages/pytriton/tri |\n",
      "|         |                                 | tonserver/backends\",\"default-ma |\n",
      "|         |                                 | x-batch-size\":\"4\"}}             |\n",
      "|         |                                 |                                 |\n",
      "|         |                                 |                                 |\n",
      "|         |                                 |                                 |\n",
      "|         |                                 |                                 |\n",
      "+---------+---------------------------------+---------------------------------+\n",
      "\n",
      "I0511 08:33:09.210924 36642 server.cc:633] \n",
      "+--------+---------+--------+\n",
      "| Model  | Version | Status |\n",
      "+--------+---------+--------+\n",
      "| AddSub | 1       | READY  |\n",
      "+--------+---------+--------+\n",
      "\n",
      "I0511 08:33:09.267927 36642 metrics.cc:864] Collecting metrics for GPU 0: Tesla V100-DGXS-32GB\n",
      "I0511 08:33:09.268315 36642 metrics.cc:757] Collecting CPU metrics\n",
      "I0511 08:33:09.268504 36642 tritonserver.cc:2264] \n",
      "+----------------------------------+------------------------------------------+\n",
      "| Option                           | Value                                    |\n",
      "+----------------------------------+------------------------------------------+\n",
      "| server_id                        | triton                                   |\n",
      "| server_version                   | 2.28.0                                   |\n",
      "| server_extensions                | classification sequence model_repository |\n",
      "|                                  |  model_repository(unload_dependents) sch |\n",
      "|                                  | edule_policy model_configuration system_ |\n",
      "|                                  | shared_memory cuda_shared_memory binary_ |\n",
      "|                                  | tensor_data statistics trace logging     |\n",
      "| model_repository_path[0]         | /home/Startupcolors/.cache/pytriton/work |\n",
      "|                                  | space_yv9yvp7s/model-store               |\n",
      "| model_control_mode               | MODE_NONE                                |\n",
      "| strict_model_config              | 0                                        |\n",
      "| rate_limit                       | OFF                                      |\n",
      "| pinned_memory_pool_byte_size     | 268435456                                |\n",
      "| cuda_memory_pool_byte_size{0}    | 67108864                                 |\n",
      "| response_cache_byte_size         | 0                                        |\n",
      "| min_supported_compute_capability | 6.0                                      |\n",
      "| strict_readiness                 | 1                                        |\n",
      "| exit_timeout                     | 30                                       |\n",
      "+----------------------------------+------------------------------------------+\n",
      "\n",
      "I0511 08:33:09.270044 36642 grpc_server.cc:4819] Started GRPCInferenceService at 0.0.0.0:8001\n",
      "I0511 08:33:09.270354 36642 http_server.cc:3477] Started HTTPService at 0.0.0.0:8000\n",
      "I0511 08:33:09.311499 36642 http_server.cc:184] Started Metrics Service at 0.0.0.0:8002\n"
     ]
    }
   ],
   "source": [
    "triton.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example inference performed with ModelClient calling triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytriton.client import ModelClient\n",
    "batch_size = 2\n",
    "a_batch = np.ones((batch_size, 1), dtype=np.float32)\n",
    "b_batch = np.ones((batch_size, 1), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during calling model callable: Traceback (most recent call last):\n",
      "  File \"/home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages/pytriton/proxy/inference_handler.py\", line 107, in run\n",
      "    outputs = self._model_callable(inputs)\n",
      "TypeError: _add_sub() takes 0 positional arguments but 1 was given\n",
      "\n"
     ]
    },
    {
     "ename": "PyTritonClientInferenceServerError",
     "evalue": "Error occurred on Triton Inference Server side:\n Failed to process the request(s) for model instance 'AddSub_0', message: TritonModelException: Traceback (most recent call last):\n  File \"/home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages/pytriton/proxy/inference_handler.py\", line 107, in run\n    outputs = self._model_callable(inputs)\nTypeError: _add_sub() takes 0 positional arguments but 1 was given\n\n\nAt:\n  /home/Startupcolors/.cache/pytriton/workspace_yv9yvp7s/model-store/AddSub/1/model.py(192): _exec_requests\n  /home/Startupcolors/.cache/pytriton/workspace_yv9yvp7s/model-store/AddSub/1/model.py(110): execute\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m~/doze/wound/notebooks/.venv/lib/python3.8/site-packages/pytriton/client/client.py:326\u001b[0m, in \u001b[0;36mModelClient._infer\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49minfer(\n\u001b[1;32m    327\u001b[0m         model_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model_name,\n\u001b[1;32m    328\u001b[0m         model_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model_version \u001b[39mor\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    329\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs_wrapped,\n\u001b[1;32m    330\u001b[0m         outputs\u001b[39m=\u001b[39;49moutputs_wrapped,\n\u001b[1;32m    331\u001b[0m         request_id\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request_id_generator)),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m \u001b[39mexcept\u001b[39;00m tritonclient\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mInferenceServerException \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/doze/wound/notebooks/.venv/lib/python3.8/site-packages/tritonclient/http/__init__.py:1512\u001b[0m, in \u001b[0;36mInferenceServerClient.infer\u001b[0;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, headers, query_params, request_compression_algorithm, response_compression_algorithm, parameters)\u001b[0m\n\u001b[1;32m   1508\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post(request_uri\u001b[39m=\u001b[39mrequest_uri,\n\u001b[1;32m   1509\u001b[0m                       request_body\u001b[39m=\u001b[39mrequest_body,\n\u001b[1;32m   1510\u001b[0m                       headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m   1511\u001b[0m                       query_params\u001b[39m=\u001b[39mquery_params)\n\u001b[0;32m-> 1512\u001b[0m _raise_if_error(response)\n\u001b[1;32m   1514\u001b[0m \u001b[39mreturn\u001b[39;00m InferResult(response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose)\n",
      "File \u001b[0;32m~/doze/wound/notebooks/.venv/lib/python3.8/site-packages/tritonclient/http/__init__.py:69\u001b[0m, in \u001b[0;36m_raise_if_error\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m error \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: [400] Failed to process the request(s) for model instance 'AddSub_0', message: TritonModelException: Traceback (most recent call last):\n  File \"/home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages/pytriton/proxy/inference_handler.py\", line 107, in run\n    outputs = self._model_callable(inputs)\nTypeError: _add_sub() takes 0 positional arguments but 1 was given\n\n\nAt:\n  /home/Startupcolors/.cache/pytriton/workspace_yv9yvp7s/model-store/AddSub/1/model.py(192): _exec_requests\n  /home/Startupcolors/.cache/pytriton/workspace_yv9yvp7s/model-store/AddSub/1/model.py(110): execute\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPyTritonClientInferenceServerError\u001b[0m        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m ModelClient(\u001b[39m\"\u001b[39m\u001b[39mlocalhost\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAddSub\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m client:\n\u001b[0;32m----> 2\u001b[0m     result_batch \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49minfer_batch(a_batch, b_batch)\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m output_name, data_batch \u001b[39min\u001b[39;00m result_batch\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00moutput_name\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mdata_batch\u001b[39m.\u001b[39mtolist()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/doze/wound/notebooks/.venv/lib/python3.8/site-packages/pytriton/client/client.py:260\u001b[0m, in \u001b[0;36mModelClient.infer_batch\u001b[0;34m(self, *inputs, **named_inputs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_supports_batching:\n\u001b[1;32m    256\u001b[0m     \u001b[39mraise\u001b[39;00m PyTritonClientModelDoesntSupportBatchingError(\n\u001b[1;32m    257\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt support batching - use infer_sample method instead\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m     )\n\u001b[0;32m--> 260\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer(inputs \u001b[39mor\u001b[39;49;00m named_inputs)\n",
      "File \u001b[0;32m~/doze/wound/notebooks/.venv/lib/python3.8/site-packages/pytriton/client/client.py:334\u001b[0m, in \u001b[0;36mModelClient._infer\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    326\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39minfer(\n\u001b[1;32m    327\u001b[0m         model_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_name,\n\u001b[1;32m    328\u001b[0m         model_version\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_version \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m         request_id\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(\u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request_id_generator)),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m \u001b[39mexcept\u001b[39;00m tritonclient\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mInferenceServerException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 334\u001b[0m     \u001b[39mraise\u001b[39;00m PyTritonClientInferenceServerError(\n\u001b[1;32m    335\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError occurred on Triton Inference Server side:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m.\u001b[39mmessage()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(response, tritonclient\u001b[39m.\u001b[39mhttp\u001b[39m.\u001b[39mInferResult):\n\u001b[1;32m    339\u001b[0m     outputs \u001b[39m=\u001b[39m {\n\u001b[1;32m    340\u001b[0m         output[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]: response\u001b[39m.\u001b[39mas_numpy(output[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mget_response()[\u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    341\u001b[0m     }\n",
      "\u001b[0;31mPyTritonClientInferenceServerError\u001b[0m: Error occurred on Triton Inference Server side:\n Failed to process the request(s) for model instance 'AddSub_0', message: TritonModelException: Traceback (most recent call last):\n  File \"/home/Startupcolors/doze/wound/notebooks/.venv/lib/python3.8/site-packages/pytriton/proxy/inference_handler.py\", line 107, in run\n    outputs = self._model_callable(inputs)\nTypeError: _add_sub() takes 0 positional arguments but 1 was given\n\n\nAt:\n  /home/Startupcolors/.cache/pytriton/workspace_yv9yvp7s/model-store/AddSub/1/model.py(192): _exec_requests\n  /home/Startupcolors/.cache/pytriton/workspace_yv9yvp7s/model-store/AddSub/1/model.py(110): execute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    }
   ],
   "source": [
    "with ModelClient(\"localhost\", \"AddSub\") as client:\n",
    "    result_batch = client.infer_batch(a_batch, b_batch)\n",
    "\n",
    "for output_name, data_batch in result_batch.items():\n",
    "    print(f\"{output_name}: {data_batch.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-setup triton server with modified inference callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine inference callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_sub(**inputs):\n",
    "    a_batch, b_batch = inputs.values()\n",
    "    add_batch = (a_batch + b_batch) * 2\n",
    "    sub_batch = (a_batch - b_batch) * 3\n",
    "    return {\"add\": add_batch, \"sub\": sub_batch}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton.bind(\n",
    "        model_name=\"AddSub\",\n",
    "        infer_func=_add_sub,\n",
    "        inputs=[\n",
    "            Tensor(dtype=np.float32, shape=(-1,)),\n",
    "            Tensor(dtype=np.float32, shape=(-1,)),\n",
    "        ],\n",
    "        outputs=[\n",
    "            Tensor(name=\"add\", dtype=np.float32, shape=(-1,)),\n",
    "            Tensor(name=\"sub\", dtype=np.float32, shape=(-1,)),\n",
    "        ],\n",
    "        config=ModelConfig(max_batch_size=128),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run triton server with new model inference callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The same inference performed with modified inference callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ModelClient(\"localhost\", \"AddSub\") as client:\n",
    "    result_batch = client.infer_batch(a_batch, b_batch)\n",
    "\n",
    "for output_name, data_batch in result_batch.items():\n",
    "    print(f\"{output_name}: {data_batch.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop server at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
